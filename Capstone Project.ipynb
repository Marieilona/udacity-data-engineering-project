{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# New York City Crime Data\n",
    "\n",
    "## Project Summary\n",
    "To mitigate crime it is essential to understand which factores drives lawless behaviour. This project creates a data model that can be used for analyzing crimes between 2012 and 2017 in New York City. The data model consist of crime complaint data, demographics of New York City's neighbourhoods and weather data. These are examples of questions that can be answered using the data model:\n",
    "* *Which borough has the most crime?*\n",
    "* *What type of crime is most common for each borough?*\n",
    "* *Does the weather effect the number of crime reports?*\n",
    "* *What charcterizes neighbourhoods with a high level of crime?*\n",
    "\n",
    "The project follows the following steps which is described further in the next sections:\n",
    "* 1. Scope the Project and Gather Data\n",
    "* 2. Explore and Assess the Data\n",
    "* 3. Define the Data Model\n",
    "* 4. Run ETL to Model the Data\n",
    "* 5. Complete Project Write Up\n",
    "\n",
    "An Jupyter Notebook running on an EMR cluster have been used to gather data from the data sources and perform the ETL process, as well as the quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 1 Project scope and data sources\n",
    "\n",
    "### 1.1 Scope \n",
    "The scope of this project is to create a data model that can be used to investigate crime statistics of New York City using weather data and demographics. \n",
    "\n",
    "### 1.2 Data sources\n",
    "This project combine data on reported crimes, weather and the demographics of New York City. The data sources are elborated on below.\n",
    "\n",
    "#### Crime Data\n",
    "The crime data is extracted from NYPD Complaint Data Historic from [NYC Open Data](https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i) and have been accessed using their API which is powered by Socrata. The dataset includes all valid felony, misdemeanor, and violation crimes reported to the New York City Police Department (NYPD) from 2006 - 2019, for this project data for 2012 - 2017 are extracted. The collected data comprises daily data on borough level.\n",
    "\n",
    "#### Demographic Data\n",
    "The demographic data consist of yearly population and income estimates on borough level and is gathered from the [American Community Survey](https://www.census.gov/data/developers/data-sets/acs-5year.html) (acs) by US Census Buerau. The data is extraced using the [census library](https://pypi.org/project/census/) which is a simple wrapper for the United States Census Bureau's API.\n",
    "\n",
    "An API key is needed to access the data. It can be retreived [here](https://www.census.gov/data/developers/guidance/api-user-guide.html)\n",
    "\n",
    "#### Weather data\n",
    "The weather data contains weather data for 36 cities around the world (including New York City) and have been downloaded from [Kaggle](https://www.kaggle.com/datasets/selfishgene/historical-hourly-weather-data?select=city_attributes.csv). The collected data comprises hourly temprature and weather descriptions on city level for 2012 to 2017.\n",
    "\n",
    "See the data dictionary in *Section 5* for a more detailed description of the used data fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"boto3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"census\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from census import Census\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# connect to Amazone S3\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "tags": [],
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-433558ee": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#### LOAD CRIME DATA ####\n",
    "\n",
    "# data source\n",
    "url = \"https://data.cityofnewyork.us/resource/qgea-i56i.json\"\n",
    "\n",
    "# query to get the correct date interval\n",
    "date_interval = \"$where=RPT_DT%20between%20'2012-01-01T00:00:00'%20and%20'2017-12-31T00:00:00'&\"\n",
    "\n",
    "# number of rows (max 50.000)\n",
    "limit = 50000\n",
    "\n",
    "# start row\n",
    "offset = 0\n",
    "\n",
    "# call api for each page\n",
    "while offset < 3000000:\n",
    "    \n",
    "    # create path\n",
    "    path = url + f\"?{date_interval}$limit={limit}&$offset={offset}\"\n",
    "\n",
    "    # read data\n",
    "    jsonData = urlopen(path).read().decode('utf-8')\n",
    "    \n",
    "    print(path)\n",
    "    \n",
    "    # stop looping when current page is empty\n",
    "    if len(jsonData) <= 3:\n",
    "        print(\"Final page reached\")\n",
    "        break;\n",
    "    \n",
    "    # print current start row\n",
    "    print(offset)\n",
    "    \n",
    "    # save data to S3\n",
    "    s3.Bucket('udacity-mikb').put_object(Key=f\"nyc_crime_data/crime_data_{offset}.json\", Body=jsonData)\n",
    "    \n",
    "    offset += 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "tags": [],
    "toggleable": true,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-297fd4be": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#### LOAD CENSUS DATA ####\n",
    "\n",
    "# set API-key\n",
    "c = Census(\"[INSERT API KEY]\")\n",
    "\n",
    "# list with relevant years\n",
    "years = [2017, 2016, 2015, 2014, 2013, 2012]\n",
    "\n",
    "# call api for all years\n",
    "for y in years:\n",
    "    # load data (returns list) \n",
    "    jsonData = c.acs5.state_county_subdivision(('NAME','B01003_001E','B05004_001E','B19013_001E','B06012_001E','B06012_002E'), '36', Census.ALL, Census.ALL, year=y)\n",
    "\n",
    "    # add year to data\n",
    "    jsonData = [dict(item, year=y) for item in jsonData]\n",
    "\n",
    "    # convert list to json format\n",
    "    jsonData = json.dumps(jsonData)\n",
    "    \n",
    "    # save data to S3\n",
    "    s3.Bucket('udacity-mikb').put_object(Key=f\"nyc_demographic_data/demographic_data_{y}.json\", Body=jsonData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2 Explore and Assess the Data\n",
    "All data sets have been explored to understand what cleaning and transformation steps that should be implemented in the ETL Pipeline.\n",
    "\n",
    "### Crime data\n",
    "The Crime Data consist of 35 columns and 2.919.428 rows. Not all columns are relvant for the final data model and must therefore be excluded while processing the data. There is no corrupted records to be adressed. However, if more data is added to the table this must be reconsidered. All columns are read as strings, hence it is necessary to cast columns containing integers and dates. All the complaint reports in the data set has an id, when including the id there is no duplicate values, however we find duplicate values when excluding the id from the table. For the purpose of the project we assume that these are seperate incidents and hence not duplicates.\n",
    "\n",
    "Columns for age group, sex and race of the suspect contains null values in around 50% of the data set. Additionally about 20% of the data set is missing the age group of the victim. It can be considered to exclude these columns due to the high amount of null values. For this case we have decided to keep them, however one must be caution to this when analyzing the data. All other relevant columns have an accepted number of null values. The table below show null values for all the extraced columns.\n",
    "\n",
    "|cmplnt_num|cmplnt_fr_dt|rpt_dt|boro_nm|ofns_desc|pd_desc|crm_atpt_cptd_cd|law_cat_cd|prem_typ_desc|susp_age_group|susp_sex|susp_race|vic_age_group|vic_race|vic_sex|\n",
    "|----------|------------|------|-------|---------|-------|----------------|----------|-------------|--------------|--------|---------|-------------|--------|-------|\n",
    "|         0|         128|     0|   2106|     5972|   2066|               2|         0|        12723|       1769362| 1226867|  1226867|       585698|       6|      6|\n",
    "\n",
    "### Demographic data\n",
    "The demographic data has 10 columns. It contains data for several neighbourhoods in the US. When extracting the neighbourhoods in New York 30 rows are left. There is no dupliactes or null values in the remaining data set. The column names are not easily understandable and must be changed to something more explanatory. Furthermore, we want to include the poverty rate of each neighbourhood which must be calculated based on column B06012_001E (population with determined poverty status) and B06012_002E (population below poverty level). Below is the original column names and their meaning. \n",
    "\n",
    "**B01003_001E:** Population  \n",
    "**B05004_001E:** Median Age  \n",
    "**B19013_001E:** Median Household Income  \n",
    "**B06012_001E:** Population whom poverty status is determined  \n",
    "**B06012_002E:** Population below poverty level  \n",
    "\n",
    "### Weather data\n",
    "The weather data is extracted from two different csv-files, one with hourly temprature data and one with hourly weather descriptions. The data set must bed joined to one weather table in the ETL pipeline. Both data sets contain data for several cities, however we only want data for New York City and must therefore drop all other columns. When only keeping New York City each of the data sets containt 2 columns and 45253 rows. The data sets has 793 missing values which make up 1.75 % of the data set. \n",
    "\n",
    "The weather data consist of hourly data, however since the crime data only have reports on daily level the weather data must be aggregated to daily level as well. To aggreagte we find the daily average temprature for each day for the temprature data, while for the weather description we use the mode for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, to_date, lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE FOR EXPLORING CRIME DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### EXPLORE CRIME DATA ####\n",
    "df = spark.read.json(\"s3://udacity-mikb/nyc_crime_data/*.json\", multiLine = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exclude columns that are not relevant\n",
    "df = df.select(\"cmplnt_num\",\n",
    "                \"cmplnt_fr_dt\",\n",
    "                \"rpt_dt\",\n",
    "                \"boro_nm\",\n",
    "                \"ofns_desc\",\n",
    "                \"pd_desc\",\n",
    "                \"crm_atpt_cptd_cd\",\n",
    "                \"law_cat_cd\",\n",
    "                \"prem_typ_desc\",\n",
    "                \"susp_age_group\",\n",
    "                \"susp_sex\",\n",
    "                \"susp_race\",\n",
    "                \"vic_age_group\",\n",
    "                \"vic_race\",\n",
    "                \"vic_sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n",
    "df.count() - df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of duplicates when excluding id column\n",
    "df.count() - df.drop(\"cmplnt_num\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of null values for each column\n",
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE FOR EXPLORING DEMOGRAPHIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### EXPLORE DEMOGRAPHIC DATA ####\n",
    "df = spark.read.json(\"s3://udacity-mikb/nyc_demographic_data/*.json\", multiLine = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only relevant cities\n",
    "df = df.filter(col(\"county subdivision\").isin({\"08510\", \"10022\", \"44919\", \"60323\", \"70915\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "df.count() - df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of null values for each column\n",
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T11:03:35.254233Z",
     "iopub.status.busy": "2022-10-27T11:03:35.253990Z",
     "iopub.status.idle": "2022-10-27T11:03:35.327016Z",
     "shell.execute_reply": "2022-10-27T11:03:35.325939Z",
     "shell.execute_reply.started": "2022-10-27T11:03:35.254201Z"
    }
   },
   "source": [
    "#### CODE FOR EXPLORING WEATHER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### EXPLORE TEMPRATURE DATA ####\n",
    "df = spark.read.csv(\"s3://udacity-mikb/nyc_weather_data/temperature.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select new york city only\n",
    "df = df.select(\"datetime\", \"new york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "df.count() - df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of null values for each column\n",
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXPLORE WEATHER DATA ####\n",
    "df = spark.read.csv(\"s3://udacity-mikb/nyc_weather_data/weather_description.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select new york city only\n",
    "df.select(\"datetime\", \"new york\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "df.count() - df.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of null values for each column\n",
    "df.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "tags": []
   },
   "source": [
    "## 3. ETL Pipeline\n",
    "The ETL pipeline creates a data lake by extracting the raw data which is was loaded to S3, clean and transform the data, and load the cleaned data back to S3. A date lake is chosen due to its ability to store big data and the schema flexibility. The data model use a star schema logic which ensures that queries are handeled efficient. The crime table act as the fact table and the weather and demograhpic tables act as dimensional tables. The demographics data and the crime tables is connected by year and neighbourhood, while the weather data can be connected using the date fields.  \n",
    "\n",
    "### How to run the ETL Pipeline\n",
    "The code in this project is set up to be run on a jupyter notebook on an EMR cluster in AWS. The notebook is installed with release 5.30.0 and the applications chosen is Spark 2.4.8 on Hadoop 2.10.1 YARN and Zeppelin 0.10.0. Data is stored on S3. When running the code the input and output files must be changed to match your preferred storage. Before running the ETL pipeline you must also retrive data from the data sources. Information on this is found in *Section 1*. When you have retrived the raw data the ETL pipeline can be run by executing the code in this section sequentially.\n",
    "\n",
    "### 3.1 Extract\n",
    "The data sets are extracted from S3 on AWS where they are stored as JSON and CSV files. \n",
    "\n",
    "### 3.2 Transform\n",
    "The extracted data is cleaned and transformed in to three tables to fit the star schema: (i) crime_table, (ii) demographic_table, (iii) weater_table. All irrelevant columns and rows have been removed, missing data and duplicates are handled, and aggregations have been performed when necessary. The column with borough names in the crime data and demographic data have been transformed to have the same formatting so that the tables can be joined together by the column. The crime table and demographic table must also use year to be joined, hence a year column have been made and extracted the report date column in the crime data. Lastly, the weather data have been aggregated on daily level so that it can be connected to the crime table through the date fields. The cleaning and tranformation steps are also described in further detail in *Section 2*.\n",
    "\n",
    "### 3.3 Load\n",
    "The transformed data get loaded back to new folders in S3 as parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date, year, regexp_extract, initcap, count, when, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = \"s3://udacity-mikb/\"\n",
    "output_data = \"s3://udacity-mikb/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to crime data file\n",
    "crime_data = os.path.join(input_data, \"nyc_crime_data/*.json\")\n",
    "    \n",
    "# read crime data file\n",
    "crime_data = spark.read.json(crime_data, multiLine = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_crime_data(input_data, output_data):\n",
    "    \"\"\"\n",
    "    Process, create tables from crime data loads data back.\n",
    "    \n",
    "    Create song and artist tables by processing song data from S3.\n",
    "    Load tables to S3 using parquet files and partitioning. \n",
    "    \n",
    "    Paramteres:\n",
    "    arg1 (SparkSession): running spark session\n",
    "    arg2 (str): part of path for input data\n",
    "    arg3 (str): part of path for output data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"processing crime data\")\n",
    "    \n",
    "    # get filepath to crime data file\n",
    "    crime_data = os.path.join(input_data, \"nyc_crime_data/*.json\")\n",
    "    \n",
    "    # read crime data file\n",
    "    crime_data = spark.read.json(crime_data, multiLine = True)\n",
    "    \n",
    "    # cast compliant number to integer\n",
    "    crime_data = crime_data.withColumn(\"cmplnt_num\", col(\"cmplnt_num\").cast(\"Integer\"))\n",
    "  \n",
    "    # cast compliant from data and report date to date\n",
    "    crime_data = crime_data.withColumn(\"cmplnt_fr_dt\", to_date(\"cmplnt_fr_dt\"))\n",
    "    crime_data = crime_data.withColumn(\"rpt_dt\", to_date(\"rpt_dt\"))\n",
    "    \n",
    "    # extract year from report date\n",
    "    crime_data = crime_data.withColumn(\"rpt_year\", year(\"rpt_dt\"))\n",
    "    \n",
    "    # capitalize borough name\n",
    "    crime_data = crime_data.withColumn(\"boro_nm\", initcap(col(\"boro_nm\")))\n",
    "    \n",
    "    # extract relevant columns and cast cmplnt_num and date variables\n",
    "    crime_table = crime_data.select(\"cmplnt_num\",\n",
    "                \"cmplnt_fr_dt\",\n",
    "                \"rpt_dt\",\n",
    "                \"rpt_year\",\n",
    "                \"boro_nm\",\n",
    "                \"ofns_desc\",\n",
    "                \"pd_desc\",\n",
    "                \"crm_atpt_cptd_cd\",\n",
    "                \"law_cat_cd\",\n",
    "                \"prem_typ_desc\",\n",
    "                \"susp_age_group\",\n",
    "                \"susp_sex\",\n",
    "                \"susp_race\",\n",
    "                \"vic_age_group\",\n",
    "                \"vic_race\",\n",
    "                \"vic_sex\").dropDuplicates([\"cmplnt_num\"])\n",
    "    \n",
    "    # load data back to S3\n",
    "    crime_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, \"nyc_crime_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_demographic_data(input_data, output_data):\n",
    "    \"\"\"\n",
    "    Process, create tables from log data and loads data back.\n",
    "    \n",
    "    Create demographic tables by processing demographic data from S3.\n",
    "    Load tables to S3 using parquet files and partitioning. \n",
    "    \n",
    "    Paramteres:\n",
    "    arg1 (SparkSession): running spark session\n",
    "    arg2 (str): part of path for input data\n",
    "    arg3 (str): part of path for output data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"processing demographic data\")\n",
    "    \n",
    "    # get filepath to demographic data file\n",
    "    demographic_table = os.path.join(input_data, \"nyc_demographic_data/*.json\")\n",
    "\n",
    "    # read demographic data file\n",
    "    demographic_table = spark.read.json(demographic_table)\n",
    "    \n",
    "    # extract neighbourhoods of new york\n",
    "    demographic_table = demographic_table \\\n",
    "    .filter(col(\"county subdivision\").isin({\"08510\", \"10022\", \"44919\", \"60323\", \"70915\"}))\n",
    "    \n",
    "    # rename columns\n",
    "    demographic_table = demographic_table.withColumnRenamed(\"B01003_001E\", \"population\")\n",
    "    demographic_table = demographic_table.withColumnRenamed(\"B19013_001E\", \"income_median\")\n",
    "    demographic_table = demographic_table.withColumnRenamed(\"B05004_001E\", \"age_median\")\n",
    "    \n",
    "    # cast year from string to int\n",
    "    demographic_table = demographic_table.withColumn(\"year\", col(\"year\").cast(\"Integer\"))\n",
    "\n",
    "    # add corrct formatting on borough column\n",
    "    demographic_table = demographic_table.withColumn(\"borough\", regexp_extract(col(\"NAME\"), \"(.+) borough\", 1))\n",
    "\n",
    "    # create poverty rate column\n",
    "    demographic_table = demographic_table.withColumn(\"poverty_rate\",\n",
    "                                                     demographic_table[\"B06012_002E\"]/demographic_table[\"B06012_001E\"])\n",
    "    \n",
    "    # select columns\n",
    "    demographic_table =  demographic_table.select(\"year\",\n",
    "                                         \"borough\",\n",
    "                                         \"population\",\n",
    "                                         \"age_median\",\n",
    "                                         \"income_median\",\n",
    "                                         \"poverty_rate\")\n",
    "    \n",
    "    # load data back to S3\n",
    "    demographic_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, \"nyc_demographic_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weather_data(input_data, output_data):\n",
    "    \"\"\"\n",
    "    Process, create tables from log data and loads data back.\n",
    "    \n",
    "    Create demographic tables by processing demographic data from S3.\n",
    "    Load tables to S3 using parquet files and partitioning. \n",
    "    \n",
    "    Paramteres:\n",
    "    arg1 (SparkSession): running spark session\n",
    "    arg2 (str): part of path for input data\n",
    "    arg3 (str): part of path for output data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"processing weather data\")\n",
    "    \n",
    "    # get filepaths to weather data file\n",
    "    temprature_data = os.path.join(input_data, \"nyc_weather_data/temperature.csv\")\n",
    "    weather_data = os.path.join(input_data, \"nyc_weather_data/weather_description.csv\")\n",
    "    \n",
    "    # load weather data files\n",
    "    temprature_data = spark.read.csv(temprature_data, header=True)\n",
    "    weather_data = spark.read.csv(weather_data, header=True)\n",
    "    \n",
    "    # select only date and new york\n",
    "    temprature_data = temprature_data.select(\"datetime\", \"new york\")\n",
    "    weather_data  = weather_data.select(\"datetime\", \"new york\")\n",
    "    \n",
    "    # create date column wihtout time stamp\n",
    "    temprature_data = temprature_data.withColumn(\"date\",to_date(\"datetime\"))\n",
    "    weather_data = weather_data.withColumn(\"date\",to_date(\"datetime\"))\n",
    "    \n",
    "    # aggregate temprature data\n",
    "    temprature_data = temprature_data.groupBy(\"date\").agg({\"new york\": \"avg\"})\n",
    "    \n",
    "    # convert degrees from kelvin to celcius\n",
    "    temprature_data = temprature_data.withColumn(\"avg_temperature\", (temprature_data[\"avg(new york)\"] - 273.15))\n",
    "                                                                                     \n",
    "    # remove column with kelvin values\n",
    "    temprature_data = temprature_data.drop(\"avg(new york)\")\n",
    "    \n",
    "    # find mode in weather description data\n",
    "    weather_data = weather_data.groupBy('date', 'new york').agg({'datetime': 'count'})\n",
    "    \n",
    "    windowDate = Window.partitionBy(\"date\").orderBy(col(\"date\").asc(), col(\"count(datetime)\").desc())\n",
    "    weather_data = weather_data.withColumn(\"row\",row_number().over(windowDate)) \\\n",
    "                  .filter(col(\"row\") == 1).drop(\"row\")\n",
    "    \n",
    "    # remove row number column and count column\n",
    "    weather_data = weather_data.drop(\"row\")\n",
    "    weather_data = weather_data.drop(\"count(datetime)\")\n",
    "    \n",
    "    # rename columns\n",
    "    weather_data = weather_data.withColumnRenamed(\"new york\", \"weather_desc\")\n",
    "    \n",
    "    # join tables\n",
    "    weather_table = temprature_data.join(weather_data, [\"date\"])\n",
    "    \n",
    "    # load data back to S3\n",
    "    weather_table.write.mode(\"overwrite\").parquet(os.path.join(output_data, \"nyc_weather_table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process crime data\n",
    "process_crime_data(input_data,output_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process crime data\n",
    "process_demographic_data(input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process crime data\n",
    "process_weather_data(input_data,output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4. Data Quality Checks\n",
    "To ensure that the piplines ran as expected a few data quality checks have been implemented. The first one checks if the files contain data, while the second one check for duplicate values. The quality checks are only performed on the weather and crime table as the demographic table is so small that it can easily be checked by viewing the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_row_count(df):\n",
    "    \"\"\"\n",
    "    Check if table is empty\n",
    "    \n",
    "    Paramteres:\n",
    "    arg1 (dataframe): table to check\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df\n",
    "\n",
    "    row_count = df.count()\n",
    "\n",
    "    if row_count == 0:\n",
    "        raise ValueError(\"Data has zero columns\")\n",
    "\n",
    "    print(f\"Data has {row_count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_value(df, from_column, to_column):\n",
    "    \"\"\"\n",
    "    Check if column has unique values\n",
    "    \n",
    "    Paramteres:\n",
    "    arg1 (dataframe): datafram to check\n",
    "    arg2 (int): from column to check\n",
    "    arg3 (int): to column to check\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df\n",
    "\n",
    "    f = from_column\n",
    "    t = to_column\n",
    "\n",
    "    no_duplicates = df.select(df.columns[f:t]).count() - df.select(df.columns[f:t]).dropDuplicates().count()\n",
    "\n",
    "    if no_duplicates > 0:\n",
    "        raise ValueError(f\"The column has {no_duplicates} duplicate values\")\n",
    "\n",
    "    print(f\"The column has unqiue values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CRIME TABLE INPUT\n",
    "df = spark.read.parquet(\"s3://udacity-mikb/nyc_crime_table/*.parquet\")\n",
    "from_column = 0\n",
    "to_column = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEATHER TABLE INPUT\n",
    "df = spark.read.parquet(\"s3://udacity-mikb/nyc_weather_table/*.parquet\")\n",
    "from_column = 0\n",
    "to_column = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN QUALITY CHECK\n",
    "check_row_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_unique_value(df, from_column, to_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 5 Data dictionary \n",
    "Below is a description of all the columns in the data model\n",
    "\n",
    "### Crime tables\n",
    "cmplnt_num: The id of complaint   \n",
    "cmplnt_fr_dt: The date of event\n",
    "rpt_dt: The date when event was reported to the police  \n",
    "rpt_year: The year when event was reported to the police  \n",
    "boro_nm: The name of the borough in which the incident occurred  \n",
    "ky_cd: Short classification of event   \n",
    "pd_cd: Long classification of event  \n",
    "crm_atpt_cptd_cd: Indicator of whether crime was successfully completed or attempted, but failed or was interrupted prematurely  \n",
    "law_cat_cd: Level of offense: felony, misdemeanor, violation  \n",
    "prem_typ_desc: Specific description of premises; grocery store, residence, street, etc.  \n",
    "susp_age_group: Age group of suspect  \n",
    "susp_sex: Sex of suspect  \n",
    "susp_race: Race of suspect  \n",
    "vic_age_group: Age group of victim  \n",
    "vic_race: Race of victim  \n",
    "vic_sex: Sex of victim  \n",
    "\n",
    "### Demograhpics table\n",
    "year: Year of estimates  \n",
    "borough: Neigbourhood in New York City  \n",
    "population: Number of inhabitants  \n",
    "age_median: Median age of popluation  \n",
    "income_median: Median income of population  \n",
    "poverty_rate: Number of people below the poverty level  \n",
    "\n",
    "### Weather table\n",
    "date: Date  \n",
    "temp: Average temprature in celcius  \n",
    "weather_desc: The weather type with highest frequency that day  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 5 Discussion of data model\n",
    "\n",
    "### 5.1 Techology choice\n",
    "This project use a date lake on the cloud platform AWS together with pysprak to ensure an efficient work flow and satisfy the need for flexibilty. The fact table of the data model contains crime data for 2012-2017 which comprises almost 3 million rows. Furthermore, the data model will be used for ad hoc analysis which requeries a great extent of flexibility both in term of querying, but also in terms of adding new data sources to the model. Through schema-on-read the chosen technology ensures the needed flexibilty for the project, while paralleism makes handeling the data load efficient. \n",
    "\n",
    "### 5.2 Updateing the data\n",
    "This data model builds upon historical data, hence there is no need to update the data model as is. However, dealing with more recent data would be useful. We suggest that the data is updated monthly or quartarly in that case as we find it most interesting to do analysis for longer timeperiods and compare the results to previous timeperiods.\n",
    "\n",
    "### Potential new user needs and requirement changes\n",
    "\n",
    "#### Increasing data by 100x\n",
    "If increasing the data by 100x one must reconsider the chosen hardware of the EMR cluster to ensure that it can handle the amount of data.\n",
    "\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "If the data is being used for a dashboard that must be updated on a daily basis at 7am every day more logic to capture potential data errors must be implemented and logged. For instance more quality checks should be added, as well as implenting logic during the processing that allow to capture potential corrupted records, missing values or duplicates that can occur when new data is added. Further more, extracting data from the sources and the ETL pipeline should be automated using AirFlow or another similar tool.\n",
    " \n",
    "#### The database needed to be accessed by 100+ people.\n",
    "If the database needs to be accessed by 100+ people one must ensure that access control is configurated properly so that each individual only has access to what they need and nothing more. Additionally, it should be reconsidered if a data lake is the right format. A data lake is very flexible, but the flexiblity in combination of many users can make the data lake unorganized and data governance issues can occur. Hence, a more strict system such as a dataware house might be better in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
